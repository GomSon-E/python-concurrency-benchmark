# 비동기 vs 동기 API 호출 성능 비교

- **총 요청 수:** 200개

---

### [1] 비동기 방식 테스트

- ✓ 완료: 200개 응답
- ⏱ 소요 시간: 2.11초

### [2] 동기 방식 테스트 중

- ✓ 완료: 20개 응답
- ⏱ 소요 시간: 12.68초 (20개 기준)

## 📊 결과 비교

| 방식 | 소요 시간 |
|------|----------|
| 비동기 (200개) | 2.11초 |
| 동기 예상 (200개) | 126.84초 |

🚀 **비동기가 약 60.0배 빠름!**

---

# 비동기 vs 멀티스레드 API 호출 성능 비교

- **총 요청 수:** 200개
- **스레드 풀 워커 수:** 50개

---

### [1] 비동기 방식 테스트

- ✓ 완료: 200개 응답
- ⏱ 소요 시간: 3.44초
- 💾 메모리: 현재 1.22MB, 피크 3.57MB

### [2] 멀티스레드 방식 테스트

- ✓ 완료: 200개 응답
- ⏱ 소요 시간: 5.61초
- 💾 메모리: 현재 0.60MB, 피크 2.91MB

## 📊 결과 비교

| 방식 | 소요 시간 | 피크 메모리 |
|------|----------|------------|
| 비동기 (asyncio + aiohttp) | 3.44초 | 3.57MB |
| 멀티스레드 (ThreadPool) | 5.61초 | 2.91MB |

- ⏱ **속도:** 비동기가 약 1.63배 빠름!
- 💾 **메모리:** 멀티스레드가 약 1.23배 적게 사용!

---

# 멀티스레드 워커 수 증가에 따른 성능 변화 테스트

## 문제 발견: 불공정한 비교

위 테스트에서 멀티스레드가 비동기보다 느렸는데, 원인을 분석해보니 **커넥션 관리 방식이 달랐다.**

| | 비동기 (aiohttp) | 멀티스레드 (requests) |
|---|---|---|
| 세션 | `ClientSession` 1개를 재사용 | **매 요청마다 새 연결 생성** |
| TCP 핸드셰이크 | 1번 (커넥션 풀링) | 200번 (매번 연결/해제) |

비동기는 세션 하나로 TCP 연결을 재사용하는데, 멀티스레드는 `requests.get()`을 호출할 때마다 새로운 TCP 연결을 맺고 끊고 있었다. 즉 매 요청마다 아래 과정이 반복된다:

1. **3-way handshake** — TCP 연결 수립. 클라이언트와 서버가 SYN → SYN-ACK → ACK 패킷 3개를 주고받아 연결을 맺는 과정
2. **TLS 협상** — HTTPS이므로 TCP 연결 위에 추가로 암호화 방식을 협상하는 과정 (인증서 검증, 키 교환 등)

세션을 재사용하면 이미 맺어진 연결을 그대로 쓰므로 이 비용이 사라진다. 200개 요청에서 이 차이가 누적되어 성능 격차가 벌어진 것이다.

## 수정 전 결과 (세션 미사용)

- **총 요청 수:** 200개
- **테스트 워커 수:** [50, 100, 200, 1000]

| 워커 수 | 소요 시간 | 피크 메모리 | 비동기 대비 |
|--------|----------|------------|------------|
| 비동기 | 2.31초 | 3.96MB | 1.00x (기준) |
| 50 | 5.53초 | 2.78MB | 2.39x |
| 100 | 5.67초 | 4.42MB | 2.45x |
| 200 | 5.77초 | 7.29MB | 2.49x |
| 1000 | 5.66초 | 7.20MB | 2.45x |

워커를 50 → 1000으로 20배 늘려도 속도가 거의 동일(약 5.6초)했다. 매 요청마다 새 연결을 맺는 비용이 병목이었기 때문에 스레드를 아무리 늘려도 소용이 없었다.

## 수정: 스레드별 세션 + HTTPAdapter

```python
thread_local = threading.local()

def get_session(pool_maxsize: int) -> requests.Session:
    if not hasattr(thread_local, "session") or thread_local.pool_maxsize != pool_maxsize:
        session = requests.Session()
        adapter = HTTPAdapter(pool_connections=10, pool_maxsize=pool_maxsize)
        session.mount("https://", adapter)
        thread_local.session = session
        thread_local.pool_maxsize = pool_maxsize
    return thread_local.session
```

- `threading.local()`로 스레드마다 독립된 세션을 생성하여 **스레드 안전** 보장
- `HTTPAdapter`의 `pool_maxsize`를 워커 수에 맞춰 설정하여 **커넥션 풀링** 적용
- `requests.Session`을 직접 공유하지 않으므로 쿠키/인증 상태 충돌 없음

## 수정 후 결과

- **총 요청 수:** 200개
- **테스트 워커 수:** [1, 2, 5, 10, 20, 50, 100]

---

### [기준] 비동기 방식 테스트

- ✓ 완료: 1.12초 | 피크 메모리: 3.81MB

### 워커 수별 멀티스레드 테스트

| 워커 수 | 소요 시간 | 피크 메모리 | 비동기 대비 |
|--------|----------|------------|------------|
| 비동기 | 1.12초 | 3.81MB | 1.00x (기준) |
| 1 | 11.06초 | 0.77MB | 9.92x |
| 2 | 9.68초 | 0.66MB | 8.68x |
| 5 | 2.39초 | 0.82MB | 2.14x |
| 10 | **1.63초** | 1.03MB | 1.46x |
| 20 | 1.71초 | 1.52MB | 1.53x |
| 50 | 1.97초 | 3.12MB | 1.77x |
| 100 | 3.23초 | 4.51MB | 2.90x |

## 📈 분석

**워커 10개가 최적점 (1.63초)이며, 그 이후로는 오히려 느려진다.**

### 워커 수를 늘려도 빨라지지 않는 이유

**1. 서버 측 동시 연결 제한 (가장 큰 원인)**

JSONPlaceholder는 무료 공개 API로 동일 클라이언트의 동시 연결 수를 제한한다. 워커가 50개여도 서버가 동시에 10~20개만 처리하면 나머지는 대기하게 되어, 유휴 스레드만 늘어난다.

**2. 스레드 오버헤드**

스레드가 늘어날수록 OS가 부담하는 비용이 증가한다:
- **컨텍스트 스위칭**: OS가 스레드 간 전환할 때마다 CPU 시간 소모
- **메모리**: 스레드당 스택 메모리 할당 (보통 8MB)
- **GIL 경합**: CPython의 GIL을 많은 스레드가 동시에 잡으려고 경쟁

**3. TCP 연결 경합**

동시 연결이 너무 많아지면 포트 고갈, 커널 소켓 버퍼 경쟁, TLS 핸드셰이크 동시 집중으로 인한 지연이 발생한다.

### 비동기가 여전히 빠른 이유

비동기는 **스레드 1개**로 200개 요청을 논블로킹으로 처리한다. 컨텍스트 스위칭, GIL 경합, 스레드 생성 비용이 전부 없기 때문에 I/O 바운드 작업에서는 워커를 아무리 늘려도 비동기를 이기기 어렵다.

> **결론**: I/O 바운드 작업에서 멀티스레드의 최적 워커 수는 **서버가 허용하는 동시 연결 수** 근처이며, 그 이상은 오버헤드만 증가한다.

---

# 멀티스레드 vs 멀티프로세스 CPU 바운드 성능 비교

- **CPU 코어 수:** 12개
- **작업 수:** 100개
- **연산 복잡도:** 50000 ~ 150000 범위 소수 계산

---

### [1] 동기 방식 (순차 실행) 테스트

- ✓ 완료: 16.75초 | 피크 메모리: 0.03MB

### [2] 멀티스레드 방식 (12 workers) 테스트

- ✓ 완료: 20.24초 | 피크 메모리: 0.32MB
- 📈 동기 대비 0.83배 속도

### [3] 멀티프로세스 방식 (12 workers) 테스트

- ✓ 완료: 3.08초 | 피크 메모리: 0.40MB
- 📈 동기 대비 5.43배 속도

## 📊 결과 요약

| 방식 | 소요 시간 | 피크 메모리 | 속도 향상 |
|-----|----------|------------|----------|
| 동기 (순차) | 16.75초 | 0.03MB | 1.00x |
| 멀티스레드 | 20.24초 | 0.32MB | 0.83x |
| 멀티프로세스 | 3.08초 | 0.40MB | 5.43x |

## 📈 분석

### GIL (Global Interpreter Lock)

Python의 GIL은 한 번에 하나의 스레드만 Python 바이트코드 실행

- **멀티스레드:** GIL로 인해 CPU 작업은 실제로 병렬 실행 안 됨
  - → 스레드 전환 오버헤드만 추가되어 오히려 느려질 수 있음
- **멀티프로세스:** 각 프로세스가 독립적인 GIL 보유
  - → 진정한 병렬 실행 가능, CPU 코어 수만큼 성능 향상

---

- 이론적 최대 속도 향상 (코어 수): **12배**
- 멀티스레드 실제 속도 향상: **0.83배** (효율: 6.9%)
- 멀티프로세스 실제 속도 향상: **5.43배** (효율: 45.3%)

⚠️ **멀티스레드는 CPU 바운드 작업에서 거의 효과 없음** (GIL 때문)

✅ **멀티프로세스가 멀티스레드보다 6.6배 빠름!**

---

# 동기가 멀티스레드보다 빠른 이유

| 방식 | 소요 시간 |
|-----|----------|
| 동기 | 16.75초 |
| 멀티스레드 | 20.24초 (오히려 3.5초 더 느림!) |

**핵심: GIL + 오버헤드**

### 동기 (순차 실행)

```
작업1 ████████ → 작업2 ████████ → 작업3 ████████
```

- GIL 경합 없음
- 컨텍스트 스위칭 없음
- 단순하게 순차 실행

### 멀티스레드 (12 workers)

```
스레드1 ██░░██░░██░░  (GIL 대기)
스레드2 ░░██░░██░░██  (GIL 대기)
스레드3 ░░░░██░░██░░  (GIL 대기)
...
```

- 12개 스레드가 GIL 하나를 두고 경쟁
- 실제로는 한 번에 1개만 실행됨
- 스레드 전환 오버헤드가 계속 발생
- 결과: 순차 실행 + 오버헤드 = 더 느림

## 멀티스레드의 오버헤드

| 오버헤드 | 설명 |
|---------|-----|
| GIL 경합 | 12개 스레드가 GIL 획득을 위해 경쟁 |
| 컨텍스트 스위칭 | OS가 스레드 간 전환하는 비용 |
| 스레드 생성/관리 | ThreadPool 관리 비용 |
| Lock 대기 | GIL 해제/획득 반복 |

## CPU 바운드 작업에서 멀티스레드는

- 병렬 실행 안 됨 (GIL 때문에 한 번에 1개만)
- 순차 실행과 동일한 작업량 + 추가 오버헤드
- 결과적으로 순차 실행보다 느림